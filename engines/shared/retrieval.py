"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π Knowledge Retrieval –¥–ª—è MCP.

–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:
- QueryExpander: —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏ –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏
- RelevanceScorer: –æ—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- SemanticDeduplicator: —É–º–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
- FallbackStrategy: —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –ø—Ä–∏ –ø—É—Å—Ç—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
- EnhancedRetrieval: –æ—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –≤—Å—ë
"""

import re
import hashlib
import asyncio
from typing import Optional, List, Tuple, Dict, Set
from dataclasses import dataclass, field

from config import get_logger
from clients import mcp_guides, mcp_knowledge

logger = get_logger(__name__)


# =============================================================================
# –°–õ–û–í–ê–†–¨ –¢–ï–†–ú–ò–ù–û–í –ò –°–ò–ù–û–ù–ò–ú–û–í
# =============================================================================

# –°–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤
# –ö–ª—é—á ‚Äî –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ—Ä–º–∏–Ω, –∑–Ω–∞—á–µ–Ω–∏—è ‚Äî —Å–≤—è–∑–∞–Ω–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è
TERM_RELATIONS: Dict[str, List[str]] = {
    # –°–æ—Å—Ç–æ—è–Ω–∏—è
    "—Ö–∞–æ—Å": ["—Ä–∞—Å—Ñ–æ–∫—É—Å", "—Ä–∞–∑–±—Ä–æ—Å–∞–Ω–Ω–æ—Å—Ç—å", "–Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω–æ–µ", "—Ñ–ª—é–≥–µ—Ä", "–≤–Ω–∏–º–∞–Ω–∏–µ"],
    "—Ç—É–ø–∏–∫": ["–∑–∞—Å—Ç—Ä–µ–≤–∞–Ω–∏–µ", "—Å—Ç–µ–∫–ª—è–Ω–Ω—ã–π –ø–æ—Ç–æ–ª–æ–∫", "–¥–µ–Ω—å —Å—É—Ä–∫–∞", "—Å—Ç–∞–≥–Ω–∞—Ü–∏—è"],
    "–ø–æ–≤–æ—Ä–æ—Ç": ["–∏–∑–º–µ–Ω–µ–Ω–∏—è", "—Å—Ç—Ä–∞—Ö –ø–µ—Ä–µ–º–µ–Ω", "–Ω–æ–≤–æ–µ –Ω–∞—á–∞–ª–æ", "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è"],
    "—Ä–∞–∑–≤–∏—Ç–∏–µ": ["—Ä–æ—Å—Ç", "–ø—Ä–æ–≥—Ä–µ—Å—Å", "–¥–≤–∏–∂–µ–Ω–∏–µ", "—É–ª—É—á—à–µ–Ω–∏–µ"],

    # –ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
    "—Å–æ–±—Ä–∞–Ω–Ω–æ—Å—Ç—å": ["—Ñ–æ–∫—É—Å", "–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è", "–≤–Ω–∏–º–∞–Ω–∏–µ", "—Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å", "–∞–≥–µ–Ω—Ç–Ω–æ—Å—Ç—å"],
    "–∞–≥–µ–Ω—Ç–Ω–æ—Å—Ç—å": ["–∞–≤—Ç–æ–Ω–æ–º–∏—è", "–¥–µ–π—Å—Ç–≤–∏–µ", "–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞", "–∫–æ–Ω—Ç—Ä–æ–ª—å", "–≤–ª–∏—è–Ω–∏–µ"],
    "–º–∞—Å—Ç–µ—Ä—Å—Ç–≤–æ": ["–Ω–∞–≤—ã–∫", "—É–º–µ–Ω–∏–µ", "–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è", "—ç–∫—Å–ø–µ—Ä—Ç–∏–∑–∞", "–ø—Ä–∞–∫—Ç–∏–∫–∞"],
    "—è—Å–Ω–æ—Å—Ç—å": ["–ø–æ–Ω–∏–º–∞–Ω–∏–µ", "—á—ë—Ç–∫–æ—Å—Ç—å", "–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å", "–≤–∏–¥–µ–Ω–∏–µ"],

    # –ü—Ä–∞–∫—Ç–∏–∫–∏
    "—Å–ª–æ—Ç —Å–∞–º–æ—Ä–∞–∑–≤–∏—Ç–∏—è": ["–≤—Ä–µ–º—è –¥–ª—è —Å–µ–±—è", "–ø—Ä–∞–∫—Ç–∏–∫–∞", "—Ä–µ–≥—É–ª—è—Ä–Ω–æ—Å—Ç—å", "—Ä–∏—Ç–º"],
    "—Ç—Ä–µ–∫–µ—Ä": ["–æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ", "—É—á—ë—Ç", "–º–µ—Ç—Ä–∏–∫–∏", "–ø—Ä–æ–≥—Ä–µ—Å—Å", "–¥–Ω–µ–≤–Ω–∏–∫"],
    "—Ä–∞–±–æ—á–∏–π –ø—Ä–æ–¥—É–∫—Ç": ["—Ä–µ–∑—É–ª—å—Ç–∞—Ç", "–∞—Ä—Ç–µ—Ñ–∞–∫—Ç", "–≤—ã—Ö–æ–¥", "deliverable"],
    "–º–µ–º": ["–µ–¥–∏–Ω–∏—Ü–∞ –º—ã—à–ª–µ–Ω–∏—è", "–∫–æ–Ω—Ü–µ–ø—Ü–∏—è", "–∏–¥–µ—è", "–ø–∞—Ç—Ç–µ—Ä–Ω"],

    # –ü—Ä–æ–±–ª–µ–º—ã
    "–ø—Ä–æ–∫—Ä–∞—Å—Ç–∏–Ω–∞—Ü–∏—è": ["–æ—Ç–∫–ª–∞–¥—ã–≤–∞–Ω–∏–µ", "–∏–∑–±–µ–≥–∞–Ω–∏–µ", "–∑–∞–≤—Ç—Ä–∞", "–ø–æ—Ç–æ–º"],
    "–≤—ã–≥–æ—Ä–∞–Ω–∏–µ": ["–∏—Å—Ç–æ—â–µ–Ω–∏–µ", "—É—Å—Ç–∞–ª–æ—Å—Ç—å", "burnout", "–ø–µ—Ä–µ–≥—Ä—É–∑–∫–∞"],
    "–±—ã—Å—Ç—Ä—ã–µ —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏—è": ["—Å–æ—Ü—Å–µ—Ç–∏", "—Å–µ—Ä–∏–∞–ª—ã", "–∏–≥—Ä—ã", "–¥–æ—Ñ–∞–º–∏–Ω", "–æ—Ç–≤–ª–µ—á–µ–Ω–∏—è"],

    # –°–∏—Å—Ç–µ–º–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ
    "—Å–∏—Å—Ç–µ–º–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ": ["—Å–∏—Å—Ç–µ–º–Ω—ã–π –ø–æ–¥—Ö–æ–¥", "—Ö–æ–ª–∏–∑–º", "–≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏", "—ç–º–µ—Ä–¥–∂–µ–Ω—Ç–Ω–æ—Å—Ç—å"],
    "—ç–∫–∑–æ–∫–æ—Ä—Ç–µ–∫—Å": ["–≤–Ω–µ—à–Ω–∏–π –º–æ–∑–≥", "–∑–∞–º–µ—Ç–∫–∏", "–±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π", "second brain"],
    "–∏—Ç–µ—Ä–∞—Ü–∏–∏": ["—Ü–∏–∫–ª—ã", "–ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è", "—Å–ø—Ä–∏–Ω—Ç—ã", "—É–ª—É—á—à–µ–Ω–∏—è"],
    "–∏–Ω–∫—Ä–µ–º–µ–Ω—Ç—ã": ["–ø—Ä–∏—Ä–∞—â–µ–Ω–∏—è", "—à–∞–≥–∏", "–º–∞–ª—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è"],

    # –†–æ–ª–∏
    "—Å–æ–∑–∏–¥–∞—Ç–µ–ª—å": ["—Ç–≤–æ—Ä–µ—Ü", "—Å–æ–∑–¥–∞—Ç–µ–ª—å", "–¥–µ—è—Ç–µ–ª—å", "maker"],
    "–ø—Ä–∞–∫—Ç–∏–∫—É—é—â–∏–π —É—á–µ–Ω–∏–∫": ["–æ—Å–æ–∑–Ω–∞–Ω–Ω—ã–π —É—á–µ–Ω–∏–∫", "—Å–∏—Å—Ç–µ–º–Ω—ã–π —É—á–µ–Ω–∏–∫"],
}

# –°–∏–Ω–æ–Ω–∏–º—ã (–¥–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω–∏–µ —Å–≤—è–∑–∏)
SYNONYMS: Dict[str, str] = {
    # –¢–µ—Ä–º–∏–Ω—ã —Å –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –Ω–∞–ø–∏—Å–∞–Ω–∏—è
    "—Å–∞–º–æ—Ä–∞–∑–≤–∏—Ç–∏–µ": "—Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–µ",
    "—Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–µ": "—Å–∞–º–æ—Ä–∞–∑–≤–∏—Ç–∏–µ",
    "—Ü–µ–ª–µ–ø–æ–ª–∞–≥–∞–Ω–∏–µ": "–ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ü–µ–ª–µ–π",
    "–ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ü–µ–ª–µ–π": "—Ü–µ–ª–µ–ø–æ–ª–∞–≥–∞–Ω–∏–µ",
    "—Ç–∞–π–º-–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç": "—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–µ–º",
    "—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–µ–º": "—Ç–∞–π–º-–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç",
    "–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å": "mindfulness",
    "mindfulness": "–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å",
    "–º–µ–¥–∏—Ç–∞—Ü–∏—è": "–ø—Ä–∞–∫—Ç–∏–∫–∞ –æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç–∏",
    "—Ñ–æ–∫—É—Å": "–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è",
    "–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è": "—Ñ–æ–∫—É—Å",
}


# =============================================================================
# DATA CLASSES
# =============================================================================

@dataclass
class RetrievalResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
    text: str
    source: str
    source_type: str  # "guides" –∏–ª–∏ "knowledge"
    relevance_score: float = 0.0
    date: Optional[str] = None
    original_item: dict = field(default_factory=dict)

    @property
    def text_hash(self) -> str:
        """–•–µ—à –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø–æ—Ö–æ–∂–µ—Å—Ç–∏"""
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        normalized = re.sub(r'\s+', ' ', self.text.lower().strip())
        return hashlib.md5(normalized[:500].encode()).hexdigest()

    @property
    def key_phrases(self) -> Set[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        text_lower = self.text.lower()
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞ –¥–ª–∏–Ω–Ω–µ–µ 4 —Å–∏–º–≤–æ–ª–æ–≤
        words = set(re.findall(r'\b[–∞-—è—ëa-z]{4,}\b', text_lower))
        return words


@dataclass
class RetrievalConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è retrieval pipeline"""
    # –õ–∏–º–∏—Ç—ã –ø–æ–∏—Å–∫–∞
    guides_limit: int = 5
    knowledge_limit: int = 5
    max_results: int = 7

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    min_relevance_score: float = 0.3

    # –†–∞–∑–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–∞
    max_chunk_size: int = 2000

    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
    similarity_threshold: float = 0.6  # Jaccard similarity –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏

    # Fallback
    enable_fallback: bool = True
    fallback_broader_query: bool = True


# =============================================================================
# QUERY EXPANDER
# =============================================================================

class QueryExpander:
    """–†–∞—Å—à–∏—Ä—è–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã —Å–≤—è–∑–∞–Ω–Ω—ã–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏"""

    def __init__(self, term_relations: Dict[str, List[str]] = None,
                 synonyms: Dict[str, str] = None):
        self.term_relations = term_relations or TERM_RELATIONS
        self.synonyms = synonyms or SYNONYMS

    def expand(self, query: str, max_expansions: int = 3) -> List[str]:
        """–†–∞—Å—à–∏—Ä—è–µ—Ç –∑–∞–ø—Ä–æ—Å —Å–≤—è–∑–∞–Ω–Ω—ã–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏

        Args:
            query: –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            max_expansions: –º–∞–∫—Å–∏–º—É–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ (–æ—Ä–∏–≥–∏–Ω–∞–ª + —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ)
        """
        queries = [query]
        query_lower = query.lower()

        expansions_added = 0

        # 1. –ò—â–µ–º –ø—Ä—è–º—ã–µ —Å–≤—è–∑–∏ —Å —Ç–µ—Ä–º–∏–Ω–∞–º–∏
        for term, related in self.term_relations.items():
            if term in query_lower:
                # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å—ã —Å —Å–≤—è–∑–∞–Ω–Ω—ã–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏
                for related_term in related[:2]:  # –ú–∞–∫—Å–∏–º—É–º 2 —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –Ω–∞ —Ç–µ—Ä–º–∏–Ω
                    if expansions_added >= max_expansions:
                        break
                    expanded = f"{query} {related_term}"
                    if expanded not in queries:
                        queries.append(expanded)
                        expansions_added += 1
                        logger.debug(f"QueryExpander: '{term}' ‚Üí –¥–æ–±–∞–≤–ª–µ–Ω '{related_term}'")

        # 2. –ò—â–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã
        for original, synonym in self.synonyms.items():
            if original in query_lower and expansions_added < max_expansions:
                # –ó–∞–º–µ–Ω—è–µ–º —Ç–µ—Ä–º–∏–Ω –Ω–∞ —Å–∏–Ω–æ–Ω–∏–º
                expanded = query_lower.replace(original, synonym)
                if expanded != query_lower and expanded not in [q.lower() for q in queries]:
                    queries.append(expanded)
                    expansions_added += 1
                    logger.debug(f"QueryExpander: —Å–∏–Ω–æ–Ω–∏–º '{original}' ‚Üí '{synonym}'")

        logger.info(f"QueryExpander: {len(queries)} –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ '{query[:50]}...'")
        return queries

    def extract_key_concepts(self, query: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞"""
        query_lower = query.lower()
        concepts = []

        for term in self.term_relations.keys():
            if term in query_lower:
                concepts.append(term)

        return concepts


# =============================================================================
# RELEVANCE SCORER
# =============================================================================

class RelevanceScorer:
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å—É"""

    def __init__(self, query_expander: QueryExpander = None):
        self.expander = query_expander or QueryExpander()

    def score(self, result: RetrievalResult, query: str,
              query_keywords: List[str] = None) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç score —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏

        Args:
            result: —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞
            query: –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            query_keywords: –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∑–∞–ø—Ä–æ—Å–∞

        Returns:
            Score –æ—Ç 0.0 –¥–æ 1.0
        """
        text_lower = result.text.lower()
        query_lower = query.lower()

        score = 0.0

        # 1. –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∑–∞–ø—Ä–æ—Å–∞ (40%)
        if query_keywords:
            matched = sum(1 for kw in query_keywords if kw.lower() in text_lower)
            keyword_score = matched / len(query_keywords) if query_keywords else 0
            score += keyword_score * 0.4

        # 2. –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π (30%)
        concepts = self.expander.extract_key_concepts(query)
        if concepts:
            concept_matches = sum(1 for c in concepts if c in text_lower)
            concept_score = concept_matches / len(concepts)
            score += concept_score * 0.3

        # 3. –ù–∞–ª–∏—á–∏–µ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ (20%)
        related_found = 0
        for concept in concepts:
            related = self.expander.term_relations.get(concept, [])
            for term in related:
                if term in text_lower:
                    related_found += 1
                    break
        if concepts:
            related_score = min(related_found / len(concepts), 1.0)
            score += related_score * 0.2

        # 4. –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ ‚Äî –±–æ–Ω—É—Å –∑–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å (10%)
        text_len = len(result.text)
        if text_len > 500:
            score += 0.1
        elif text_len > 200:
            score += 0.05

        # 5. –®—Ç—Ä–∞—Ñ –∑–∞ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç
        if text_len < 100:
            score *= 0.5

        return min(score, 1.0)

    def rank_results(self, results: List[RetrievalResult], query: str,
                     query_keywords: List[str] = None) -> List[RetrievalResult]:
        """–†–∞–Ω–∂–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
        for result in results:
            result.relevance_score = self.score(result, query, query_keywords)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ score (—É–±—ã–≤–∞–Ω–∏–µ)
        ranked = sorted(results, key=lambda r: r.relevance_score, reverse=True)

        logger.info(f"RelevanceScorer: —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–æ {len(ranked)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, "
                   f"top score={ranked[0].relevance_score:.2f}" if ranked else "")

        return ranked


# =============================================================================
# SEMANTIC DEDUPLICATOR
# =============================================================================

class SemanticDeduplicator:
    """–£–º–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞"""

    def __init__(self, similarity_threshold: float = 0.6):
        self.similarity_threshold = similarity_threshold

    def jaccard_similarity(self, set1: Set[str], set2: Set[str]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ñ–∞–∫–∫–∞—Ä–∞ –º–µ–∂–¥—É –¥–≤—É–º—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞–º–∏"""
        if not set1 or not set2:
            return 0.0
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        return intersection / union if union > 0 else 0.0

    def are_similar(self, result1: RetrievalResult, result2: RetrievalResult) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è—é—Ç—Å—è –ª–∏ –¥–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏–º–∏"""
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Ö–µ—à—É (–∏–¥–µ–Ω—Ç–∏—á–Ω—ã–π —Ç–µ–∫—Å—Ç)
        if result1.text_hash == result2.text_hash:
            return True

        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Ñ—Ä–∞–∑–∞–º (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ)
        phrases1 = result1.key_phrases
        phrases2 = result2.key_phrases

        similarity = self.jaccard_similarity(phrases1, phrases2)

        return similarity >= self.similarity_threshold

    def deduplicate(self, results: List[RetrievalResult]) -> List[RetrievalResult]:
        """–£–¥–∞–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

        –û—Å—Ç–∞–≤–ª—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –Ω–∞–∏–≤—ã—Å—à–∏–º relevance_score –∏–∑ –≥—Ä—É–ø–ø—ã –ø–æ—Ö–æ–∂–∏—Ö.
        """
        if not results:
            return []

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ score —á—Ç–æ–±—ã –æ—Å—Ç–∞–≤–ª—è—Ç—å –ª—É—á—à–∏–µ
        sorted_results = sorted(results, key=lambda r: r.relevance_score, reverse=True)

        unique_results = []

        for result in sorted_results:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—Ö–æ–∂ –ª–∏ –Ω–∞ —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–µ
            is_duplicate = False
            for existing in unique_results:
                if self.are_similar(result, existing):
                    is_duplicate = True
                    logger.debug(f"Deduplicator: –ø—Ä–æ–ø—É—â–µ–Ω –¥—É–±–ª—å "
                               f"(similarity >= {self.similarity_threshold})")
                    break

            if not is_duplicate:
                unique_results.append(result)

        removed = len(results) - len(unique_results)
        if removed > 0:
            logger.info(f"SemanticDeduplicator: —É–¥–∞–ª–µ–Ω–æ {removed} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")

        return unique_results


# =============================================================================
# FALLBACK STRATEGY
# =============================================================================

class FallbackStrategy:
    """–°—Ç—Ä–∞—Ç–µ–≥–∏—è –ø—Ä–∏ –ø—É—Å—Ç—ã—Ö –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö"""

    def __init__(self, expander: QueryExpander = None):
        self.expander = expander or QueryExpander()

    def generate_fallback_queries(self, original_query: str,
                                  tried_queries: List[str]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç fallback –∑–∞–ø—Ä–æ—Å—ã

        Args:
            original_query: –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            tried_queries: —É–∂–µ –æ–ø—Ä–æ–±–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã

        Returns:
            –ù–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –ø–æ–ø—ã—Ç–∫–∏
        """
        fallback_queries = []
        tried_set = set(q.lower() for q in tried_queries)

        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏ –∏—â–µ–º –ø–æ –Ω–∏–º –æ—Ç–¥–µ–ª—å–Ω–æ
        concepts = self.expander.extract_key_concepts(original_query)
        for concept in concepts:
            if concept.lower() not in tried_set:
                fallback_queries.append(concept)

        # 2. –ë–µ—Ä—ë–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –Ω–∞–ø—Ä—è–º—É—é
        for concept in concepts:
            related = self.expander.term_relations.get(concept, [])
            for term in related[:2]:
                if term.lower() not in tried_set:
                    fallback_queries.append(term)

        # 3. –£–ø—Ä–æ—â–∞–µ–º –∑–∞–ø—Ä–æ—Å ‚Äî –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ (–¥–ª–∏–Ω–Ω—ã–µ —Å–ª–æ–≤–∞)
        words = re.findall(r'\b[–∞-—è—ëa-z]{5,}\b', original_query.lower())
        simple_query = ' '.join(words[:3])
        if simple_query and simple_query not in tried_set:
            fallback_queries.append(simple_query)

        logger.info(f"FallbackStrategy: —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(fallback_queries)} –∑–∞–ø—Ä–æ—Å–æ–≤")
        return fallback_queries[:3]  # –ú–∞–∫—Å–∏–º—É–º 3 fallback –∑–∞–ø—Ä–æ—Å–∞


# =============================================================================
# ENHANCED RETRIEVAL ‚Äî –ì–õ–ê–í–ù–´–ô –ö–õ–ê–°–°
# =============================================================================

class EnhancedRetrieval:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π Knowledge Retrieval —Å –ø–æ–ª–Ω—ã–º pipeline"""

    def __init__(self, config: RetrievalConfig = None):
        self.config = config or RetrievalConfig()
        self.expander = QueryExpander()
        self.scorer = RelevanceScorer(self.expander)
        self.deduplicator = SemanticDeduplicator(self.config.similarity_threshold)
        self.fallback = FallbackStrategy(self.expander)

    async def search(self, query: str,
                     keywords: List[str] = None,
                     context_topic: Optional[str] = None,
                     dynamic_context: "DynamicContext" = None) -> Tuple[str, List[str]]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ MCP

        Args:
            query: –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            keywords: –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–µ—Å–ª–∏ —É–∂–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã)
            context_topic: –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç–µ–∫—É—â–µ–π —Ç–µ–º—ã
            dynamic_context: –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–ø—Ä–æ–≥—Ä–µ—Å—Å, –∏—Å—Ç–æ—Ä–∏—è, –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)

        Returns:
            Tuple[context, sources] - –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM –∏ —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        """
        logger.info(f"EnhancedRetrieval: –∑–∞–ø—Ä–æ—Å '{query[:80]}...'")

        # 1. –§–æ—Ä–º–∏—Ä—É–µ–º –±–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        base_query = f"{context_topic} {query}" if context_topic else query

        # 2. –î–æ–±–∞–≤–ª—è–µ–º boost_concepts –∏–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        boost_terms = []
        if dynamic_context:
            boost_terms = dynamic_context.get_search_boost_terms()
            if boost_terms:
                # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∫ –∑–∞–ø—Ä–æ—Å—É
                boost_query = ' '.join(boost_terms[:3])
                base_query = f"{base_query} {boost_query}"
                logger.info(f"EnhancedRetrieval: boost terms: {boost_terms[:3]}")

        # 3. –†–∞—Å—à–∏—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å—ã
        expanded_queries = self.expander.expand(base_query, max_expansions=2)

        # 3. –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –∑–∞–ø—Ä–æ—Å–∞–º
        all_results: List[RetrievalResult] = []
        tried_queries = []

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –ø–æ –í–°–ï–ú expanded queries –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–û
        search_tasks = [self._search_both_sources(q) for q in expanded_queries]
        search_results = await asyncio.gather(*search_tasks)

        for results in search_results:
            all_results.extend(results)
        tried_queries.extend(expanded_queries)

        # 4. Fallback —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å–æ–≤—Å–µ–º –º–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–Ω–µ 3, –∞ 1)
        if len(all_results) < 2 and self.config.enable_fallback:
            logger.info("EnhancedRetrieval: –æ—á–µ–Ω—å –º–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –ø—Ä–æ–±—É–µ–º fallback")
            fallback_queries = self.fallback.generate_fallback_queries(
                base_query, tried_queries
            )[:2]  # –ú–∞–∫—Å–∏–º—É–º 2 fallback –∑–∞–ø—Ä–æ—Å–∞
            if fallback_queries:
                fallback_results = await asyncio.gather(
                    *[self._search_both_sources(q) for q in fallback_queries]
                )
                for results in fallback_results:
                    all_results.extend(results)

        # 5. Scoring
        all_results = self.scorer.rank_results(all_results, base_query, keywords)

        # 6. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É score
        filtered = [r for r in all_results
                   if r.relevance_score >= self.config.min_relevance_score]

        if len(filtered) < len(all_results):
            logger.info(f"EnhancedRetrieval: –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(all_results) - len(filtered)} "
                       f"—Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –Ω–∏–∑–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é")

        # 7. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        unique_results = self.deduplicator.deduplicate(filtered)

        # 8. –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        final_results = unique_results[:self.config.max_results]

        # 9. –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        context, sources = self._format_results(final_results)

        logger.info(f"EnhancedRetrieval: –∏—Ç–æ–≥–æ {len(final_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, "
                   f"{len(context)} —Å–∏–º–≤–æ–ª–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")

        return context, sources

    async def _search_both_sources(self, query: str) -> List[RetrievalResult]:
        """–ò—â–µ—Ç –≤ –æ–±–æ–∏—Ö MCP –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–û"""
        results = []

        async def search_guides():
            """–ü–æ–∏—Å–∫ –≤ MCP-Guides"""
            try:
                return await mcp_guides.semantic_search(
                    query, lang="ru", limit=self.config.guides_limit
                )
            except Exception as e:
                logger.error(f"MCP-Guides error: {e}")
                return []

        async def search_knowledge():
            """–ü–æ–∏—Å–∫ –≤ MCP-Knowledge"""
            try:
                return await mcp_knowledge.search(
                    query, limit=self.config.knowledge_limit
                )
            except Exception as e:
                logger.error(f"MCP-Knowledge error: {e}")
                return []

        # –í—ã–ø–æ–ª–Ω—è–µ–º –æ–±–∞ –∑–∞–ø—Ä–æ—Å–∞ –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–û
        guides_results, knowledge_results = await asyncio.gather(
            search_guides(),
            search_knowledge()
        )

        # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã Guides
        for item in (guides_results or []):
            result = self._parse_result(item, "guides")
            if result:
                results.append(result)

        # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã Knowledge
        for item in (knowledge_results or []):
            result = self._parse_result(item, "knowledge")
            if result:
                results.append(result)

        return results

    def _parse_result(self, item: dict, source_type: str) -> Optional[RetrievalResult]:
        """–ü–∞—Ä—Å–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ MCP –≤ RetrievalResult"""
        if isinstance(item, str):
            text = item
            source = "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã Aisystant"  # –î–µ—Ñ–æ–ª—Ç–Ω–æ–µ –∏–º—è –≤–º–µ—Å—Ç–æ Unknown
            date = None
        elif isinstance(item, dict):
            # –õ–æ–≥–∏—Ä—É–µ–º –∫–ª—é—á–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ (—Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π —Ä–∞–∑)
            if not hasattr(self, '_logged_keys'):
                self._logged_keys = set()
            item_keys = frozenset(item.keys())
            if item_keys not in self._logged_keys:
                logger.debug(f"MCP {source_type} item keys: {list(item.keys())}")
                self._logged_keys.add(item_keys)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            text = item.get('text', item.get('content', item.get('snippet', '')))

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ ‚Äî –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–ª—é—á–∏
            source = (
                item.get('source') or
                item.get('guide') or
                item.get('guide_title') or
                item.get('guide_slug') or
                item.get('section') or
                item.get('section_title') or
                item.get('title') or
                item.get('name') or
                item.get('url') or
                # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ‚Äî –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ —Å–ª–æ–≤–∞ —Ç–µ–∫—Å—Ç–∞
                (text[:50].split('.')[0] if text else None) or
                "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã Aisystant"
            )

            date = item.get('created_at', item.get('date', item.get('published_at')))
        else:
            return None

        if not text or len(text.strip()) < 50:
            return None

        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
        text = text[:self.config.max_chunk_size]

        return RetrievalResult(
            text=text,
            source=source,
            source_type=source_type,
            date=date,
            original_item=item if isinstance(item, dict) else {}
        )

    def _format_results(self, results: List[RetrievalResult]) -> Tuple[str, List[str]]:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM"""
        if not results:
            return "", []

        context_parts = []
        sources = []

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        guides_results = [r for r in results if r.source_type == "guides"]
        knowledge_results = [r for r in results if r.source_type == "knowledge"]

        # Knowledge –∏–º–µ–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (—Å–≤–µ–∂–∏–µ –ø–æ—Å—Ç—ã –≤–∞–∂–Ω–µ–µ)
        if knowledge_results:
            context_parts.append("üìö –ê–ö–¢–£–ê–õ–¨–ù–´–ï –ú–ê–¢–ï–†–ò–ê–õ–´:")
            for r in knowledge_results:
                date_prefix = f"[{r.date}] " if r.date else ""
                context_parts.append(f"{date_prefix}{r.text}")
                if r.source and f"–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π: {r.source}" not in sources:
                    sources.append(f"–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π: {r.source}")

        if guides_results:
            if knowledge_results:
                context_parts.append("\n---\n")
            context_parts.append("üìñ –ò–ó –†–£–ö–û–í–û–î–°–¢–í:")
            for r in guides_results:
                context_parts.append(r.text)
                if r.source and f"–†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ: {r.source}" not in sources:
                    sources.append(f"–†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ: {r.source}")

        context = "\n\n".join(context_parts)

        return context, sources


# =============================================================================
# –£–î–û–ë–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø
# =============================================================================

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π instance
_retrieval_instance: Optional[EnhancedRetrieval] = None


def get_retrieval() -> EnhancedRetrieval:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç singleton instance EnhancedRetrieval"""
    global _retrieval_instance
    if _retrieval_instance is None:
        _retrieval_instance = EnhancedRetrieval()
    return _retrieval_instance


async def enhanced_search(query: str,
                         keywords: List[str] = None,
                         context_topic: Optional[str] = None,
                         dynamic_context: "DynamicContext" = None) -> Tuple[str, List[str]]:
    """–£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞

    Args:
        query: –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        keywords: –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        context_topic: –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç–µ–º—ã
        dynamic_context: –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

    Returns:
        Tuple[context, sources]
    """
    retrieval = get_retrieval()
    return await retrieval.search(query, keywords, context_topic, dynamic_context)


# Type hint import (–≤ –∫–æ–Ω—Ü–µ —Ñ–∞–π–ª–∞ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏—Ö –∏–º–ø–æ—Ä—Ç–æ–≤)
try:
    from .context import DynamicContext
except ImportError:
    DynamicContext = None
